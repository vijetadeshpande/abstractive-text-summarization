{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstractive Text Summarization\n",
    "===\n",
    "This notebook is an in-progress implementation/experiment of the [Abstractive Text Summarization using Sequence-to-sequence RNNs and\n",
    "Beyond](https://arxiv.org/abs/1602.06023) paper.\n",
    "\n",
    "Current Features\n",
    "---\n",
    "* model architecture supports LSTM & GRU (biLSTM-to-uniLSTM or biGRU-to-uniGRU)\n",
    "* implements batch data processing \n",
    "* implements attention mechanism ([Bahdanau et al.](https://arxiv.org/abs/1409.0473) & [Luong et al.(global dot)](https://arxiv.org/abs/1508.04025))\n",
    "* implements [scheduled sampling (teacher forcing)](https://arxiv.org/abs/1506.03099)\n",
    "* implements [tied embeddings](https://arxiv.org/pdf/1608.05859.pdf)\n",
    "* initializes encoder-decoder with pretrained vectors (glove.6B.200d)\n",
    "* implements custom training callbacks (tensorboard visualization for PyTorch, save best model & log checkpoint)\n",
    "* implements attention plots\n",
    "\n",
    "\n",
    "To-Do\n",
    "---\n",
    "* Implement additional linguistic features embeddings  \n",
    "* Implement generator-pointer switch and replace unknown words by selecting source token with the highest attention score.\n",
    "* Implement large vocabulary trick \n",
    "* Implement sentence level attention \n",
    "* Implement beam search during inference\n",
    "* implement rouge evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements\n",
    "---\n",
    "\n",
    "1. Create conda environment \n",
    "\n",
    "`conda env create -f environment.yml`  --gpu\n",
    "\n",
    "`conda env create -f environment-cpu.yml`  --cpu\n",
    "\n",
    "2. Install dependencies (PyTorch, Fastai, TorchText, Tensorboard etc) via:\n",
    "\n",
    "`pip install -r requirements.txt`\n",
    "\n",
    "3. Download `spacy` english module\n",
    "\n",
    "`python -m spacy download en`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset\n",
    "--\n",
    "\n",
    "The dataset used is a subset of the gigaword dataset and can be found [here](https://drive.google.com/file/d/0B6N7tANPyVeBNmlSX19Ld2xDU1E/view?usp=sharing).\n",
    "\n",
    "It contains 3,803,955 parallel source & target examples for training and 189,649 examples for validation.\n",
    "\n",
    "After downloading, we create article-title pairs, save in tabular datset format (.csv) and extract a sample subset (80,000 for training & 20,000 for validation). This data preparation can be found [here](/data-preparation.ipynb).\n",
    "\n",
    "An example article-title pair looks like this:\n",
    "\n",
    "`article: the algerian cabinet chaired by president abdelaziz bouteflika on sunday adopted the #### finance bill predicated on an oil price of ## dollars a barrel and a growth rate of #.# percent , it was announced here .`\n",
    "\n",
    "`title: algeria adopts #### finance bill with oil put at ## dollars a barrel`\n",
    "\n",
    "\n",
    "Training on the complete dataset (3M) would take a really long time. So in order to train and experiment faster we use our sample subset of 80,000 in this tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE_GPU=False\n"
     ]
    }
   ],
   "source": [
    "#declare the directory path to dataset  \n",
    "DATA_PATH = 'data/'\n",
    "SAMPLE_DATA_PATH = f'{DATA_PATH}sample_data/'\n",
    "PROCESSED_DATA_PATH = f'{DATA_PATH}processed_data/'\n",
    "\n",
    "#Enable GPU training \n",
    "import torch\n",
    "USE_GPU = False#torch.cuda.is_available()\n",
    "print('USE_GPU={}'.format(USE_GPU))\n",
    "if USE_GPU:\n",
    "    print('current_device={}'.format(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 1. Process dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order to train, we performed common processing steps on the dataset such as:\n",
    "\n",
    "* Loading dataset\n",
    "* Preprocessing dataset (tokenizing, appending begining-of-sentence and end-of-sentence tokens, truncating, etc)\n",
    "* Building a vocabulary\n",
    "* Creating dataset iterators\n",
    "* Batching, padding, and numericalizing. \n",
    "\n",
    "To process the dataset, we use the [torchtext](https://github.com/pytorch/text) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#import data, vocab from torchtext \n",
    "from torchtext import data, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.1. Load & define preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To pre-process our data, we declare a `Field` class, and pass additional pre-processing arguments ( ex. tokenize with the `spacy` tokenizer, `lower` and append an `end-of-sentence` token to every example )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tokenizer = data.get_tokenizer('spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize=tokenizer, lower=True, eos_token='_eos_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next, we load our training & validation tabular dataset using `data.TabularDataset.splits` which applies the defined preprocessing pipeline and returns their respective `Dataset` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54 s, sys: 247 ms, total: 54.2 s\n",
      "Wall time: 54.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trn_data_fields = [(\"source\", TEXT),\n",
    "                   (\"target\", TEXT)]\n",
    "\n",
    "trn, vld = data.TabularDataset.splits(path=f'{SAMPLE_DATA_PATH}',\n",
    "                                     train='train_ds.csv', validation='valid_ds.csv',\n",
    "                                     format='csv', skip_header=True, fields=trn_data_fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jason', 'blake', 'of', 'the', 'islanders', 'will', 'miss', 'the', 'rest', 'of', 'the', 'season', 'so', 'he', 'can', 'be', 'with', 'his', 'wife', ',', 'who', 'has', 'thyroid', 'cancer', 'and', 'is', 'to', 'give', 'birth', 'april', '#', '.'] ['blake', 'missing', 'rest', 'of', 'season']\n"
     ]
    }
   ],
   "source": [
    "# a sample of the preprocessed data\n",
    "#trn = trn[0:100]\n",
    "#vld = vld[0:20]\n",
    "print(trn[0].source, trn[0].target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.2. Build vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Building a vocabulary simply means mapping each unique token in the corpus to an integer value, and storing as a dictionary ex `{'the': 2, 'brown':3, 'fox':4}`. \n",
    "\n",
    "In addition to building a vocabulary, we also use torchtext to load an embedding matrix for each token using the `glove.6B.200d` pretrained vector.\n",
    "\n",
    "We pass to `TEXT.build_vocab` our training dataset object `trn` and also the name of the pretrained vector we would like to use `glove.6B.200d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.44 s, sys: 261 ms, total: 1.7 s\n",
      "Wall time: 1.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pre_trained_vector_type = 'glove.6B.200d' \n",
    "TEXT.build_vocab(trn, vectors=pre_trained_vector_type )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 152957),\n",
       " ('the', 130459),\n",
       " ('.', 105054),\n",
       " (',', 85497),\n",
       " ('to', 83508),\n",
       " ('in', 78169),\n",
       " ('of', 77424),\n",
       " ('a', 71025),\n",
       " ('on', 43536),\n",
       " ('and', 42555)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10 most frequent words in the vocab\n",
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.3. Create dataset iterator, batch, pad, and numericalize. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next, create a training & validation iterator object, numericalize (turn text to tensors), batch examples of similar lengths together, randomly shuffle the data and pad tensors using `data.BucketIterator.splits`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "train_iter, val_iter = data.BucketIterator.splits(\n",
    "                        (trn, vld), batch_sizes=(batch_size, int(batch_size*1.6)),\n",
    "                        device=(0 if USE_GPU else -1), \n",
    "                        sort_key=lambda x: len(x.source),\n",
    "                        shuffle=True, sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next, we stick each article-title batch pair tensor into a tuple (article, title). To do this we create a custom helper class `BatchTuple`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class BatchTuple():\n",
    "    def __init__(self, dataset, x_var, y_var):\n",
    "        self.dataset, self.x_var, self.y_var = dataset, x_var, y_var\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for batch in self.dataset:\n",
    "            x = getattr(batch, self.x_var) \n",
    "            y = getattr(batch, self.y_var)                 \n",
    "            yield (x, y)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#returns tuple of article-title pair tensors\n",
    "train_iter_tuple = BatchTuple(train_iter, \"source\", \"target\")\n",
    "val_iter_tuple = BatchTuple(val_iter, \"source\", \"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  10, 6241,    4,  ...,  242,   93,  154],\n",
       "         [ 560, 6629, 1351,  ..., 1128,  378,   98],\n",
       "         [1647,   13, 2816,  ...,  148, 3299,  367],\n",
       "         ...,\n",
       "         [   1,  246,    1,  ...,    1,    1,    1],\n",
       "         [   1,    5,    1,  ...,    1,    1,    1],\n",
       "         [   1,    2,    1,  ...,    1,    1,    1]]),\n",
       " tensor([[   3, 6241, 1351,  ...,  475,   93, 1382],\n",
       "         [   3, 6629, 1487,  ...,   14, 9571, 3036],\n",
       "         [2204, 1970, 1296,  ...,   42, 2046,    8],\n",
       "         ...,\n",
       "         [   1,    1,    1,  ...,  182,    1,    1],\n",
       "         [   1,    1,    1,  ..., 5017,    1,    1],\n",
       "         [   1,    1,    1,  ...,    2,    1,    1]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#an example of a batched and padded article-title tensor pair\n",
    "next(iter(train_iter_tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.4. Create ModelData "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order to use our batched dataset with the FastAI library, we create a `ModelData` object. \n",
    "\n",
    "`ModelData` simply sticks the training, validation (and test) dataset into a single object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelData():\n",
    "    \"\"\"Encapsulates DataLoaders and Datasets for training, validation, test. Base class for fastai *Data classes.\"\"\"\n",
    "    def __init__(self, path, trn_dl, val_dl, test_dl=None):\n",
    "        self.path,self.trn_dl,self.val_dl,self.test_dl = path,trn_dl,val_dl,test_dl\n",
    "\n",
    "    @classmethod\n",
    "    def from_dls(cls, path,trn_dl,val_dl,test_dl=None):\n",
    "        #trn_dl,val_dl = DataLoader(trn_dl),DataLoader(val_dl)\n",
    "        #if test_dl: test_dl = DataLoader(test_dl)\n",
    "        return cls(path, trn_dl, val_dl, test_dl)\n",
    "\n",
    "    @property\n",
    "    def is_reg(self): return self.trn_ds.is_reg\n",
    "    @property\n",
    "    def is_multi(self): return self.trn_ds.is_multi\n",
    "    @property\n",
    "    def trn_ds(self): return self.trn_dl.dataset\n",
    "    @property\n",
    "    def val_ds(self): return self.val_dl.dataset\n",
    "    @property\n",
    "    def test_ds(self): return self.test_dl.dataset\n",
    "    @property\n",
    "    def trn_y(self): return self.trn_ds.y\n",
    "    @property\n",
    "    def val_y(self): return self.val_ds.y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#import text module from fastai \n",
    "import sys\n",
    "sys.path.insert(0, \"/Users/vijetadeshpande/Documents/GitHub/fastai/old\")\n",
    "#from fastai import text\n",
    "import fastai\n",
    "import numpy as np\n",
    "#from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_data = ModelData(SAMPLE_DATA_PATH, trn_dl=train_iter_tuple, val_dl=val_iter_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1.5. Processed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, we are done with processing the dataset: pre-processing, numericalizing, batching and padding.\n",
    "\n",
    "Lets take a look at the final processed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 197, 52299)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of batches in training & validation set and number of tokens in vocabulary\n",
    "len(model_data.trn_dl), len(model_data.val_dl), len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([53, 64]), torch.Size([20, 64]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of one batch in training set (sequence_length x batch_size)\n",
    "t, z = next(model_data.trn_dl.__iter__())\n",
    "t.size(), z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\n",
      "gold futures on the comex division of the new york mercantile exchange regained some ground on friday . _eos_ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n",
      "corresponding tensor:\n",
      "[  280  1055    11     4 11655  1135     9     4    28   218  7685   228\n",
      "  7281   171  1029    11    40     5     2     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1] \n",
      "\n",
      "target:\n",
      "gold bounces off on bargain hunting _eos_ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
      "\n",
      "corresponding tensor:\n",
      "[  280 10469   128    11  3385  2666     2     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lets look at an example pair\n",
    "sample_source = t.transpose(1,0)[0].data.cpu().numpy()\n",
    "sample_target = z.transpose(1,0)[0].data.cpu().numpy()\n",
    "\n",
    "print(\"source:\\n%s \\n\\ncorresponding tensor:\\n%s \\n\" %(' '.join([TEXT.vocab.itos[o] for o in sample_source]), sample_source))\n",
    "print(\"target:\\n%s \\n\\ncorresponding tensor:\\n%s \\n\" %(' '.join([TEXT.vocab.itos[o] for o in sample_target]), sample_target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Define model architecture\n",
    "The sequence model, consists of:\n",
    "* single layer encoder-decoder RNNs (biGRU-to-uniGRU)\n",
    "* feed-forward attention network (bahdanau)\n",
    "* option for luong global dot attention\n",
    "* option for teacher forcing\n",
    "* option for tied embeddings\n",
    "* option for multi-layer model\n",
    "* option for regularization (dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import _pickle as pickle\n",
    "from functools import partial\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, rnn_type, input_size, embz_size, hidden_size, batch_size,output_size,max_tgt_len,\n",
    "                 attention_type, tied_weight_type, pre_trained_vector, pre_trained_vector_type, padding_idx,\n",
    "                 num_layers=1, encoder_drop=(0.0,0.0), decoder_drop=(0.0,0.0), \n",
    "                 bidirectional=True, bias=False, teacher_forcing=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        rnn_type, attention_type, tied_weight_type = rnn_type.upper(), attention_type.title(), tied_weight_type.lower()\n",
    "        \n",
    "        if rnn_type in ['LSTM', 'GRU']: self.rnn_type = rnn_type\n",
    "        else: raise ValueError(\"\"\"An invalid option for '--rnn_type' was supplied,\n",
    "                                    options are ['LSTM', 'GRU']\"\"\")\n",
    "            \n",
    "        if attention_type in ['Luong', 'Bahdanau']: self.attention_type = attention_type\n",
    "        else: raise ValueError(\"\"\"An invalid option for '--attention_type' was supplied,\n",
    "                                    options are ['Luong', 'Bahdanau']\"\"\")\n",
    "            \n",
    "        if tied_weight_type in ['three_way', 'two_way']: self.tied_weight_type = tied_weight_type\n",
    "        else: raise ValueError(\"\"\"An invalid option for '--tied_weight_type' was supplied,\n",
    "                                    options are ['three_way', 'two_way']\"\"\")\n",
    "    \n",
    "                    \n",
    "        #initialize model parameters            \n",
    "        self.output_size, self.embz_size, self.hidden_size = output_size, embz_size, hidden_size//2\n",
    "        self.num_layers, self.input_size, self.max_tgt_len, self.pre_trained_vector = num_layers, input_size, max_tgt_len, pre_trained_vector\n",
    "        self.bidirectional,self.teacher_forcing, self.pre_trained_vector_type = bidirectional, teacher_forcing, pre_trained_vector_type\n",
    "        self.encoder_drop, self.decoder_drop, self.padding_idx = encoder_drop, decoder_drop, padding_idx\n",
    "        \n",
    "        \n",
    "        if self.teacher_forcing: self.force_prob = 1.0\n",
    "        \n",
    "        #set bidirectional\n",
    "        if self.bidirectional: self.num_directions = 2\n",
    "        else: self.num_directions = 1\n",
    "            \n",
    "        \n",
    "        #encoder\n",
    "        self.encoder_dropout = nn.Dropout(self.encoder_drop[0])\n",
    "        self.encoder_embedding_layer = nn.Embedding(self.input_size, self.embz_size, padding_idx=self.padding_idx)\n",
    "        if self.pre_trained_vector: self.encoder_embedding_layer.weight.data.copy_(self.pre_trained_vector.weight.data)\n",
    "            \n",
    "        self.encoder_rnn = getattr(nn, self.rnn_type)(\n",
    "                           input_size=self.embz_size,\n",
    "                           hidden_size=self.hidden_size,\n",
    "                           num_layers=self.num_layers,\n",
    "                           dropout=self.encoder_drop[1], \n",
    "                           bidirectional=self.bidirectional)\n",
    "        self.encoder_vector_layer = nn.Linear(self.hidden_size*self.num_directions,self.embz_size, bias=bias)\n",
    "        \n",
    "       #decoder\n",
    "        self.decoder_dropout = nn.Dropout(self.decoder_drop[0])\n",
    "        self.decoder_embedding_layer = nn.Embedding(self.input_size, self.embz_size, padding_idx=self.padding_idx)\n",
    "        self.decoder_rnn = getattr(nn, self.rnn_type)(\n",
    "                           input_size=self.embz_size,\n",
    "                           hidden_size=self.hidden_size*self.num_directions,\n",
    "                           num_layers=self.num_layers,\n",
    "                           dropout=self.decoder_drop[1]) \n",
    "        self.decoder_output_layer = nn.Linear(self.hidden_size*self.num_directions, self.embz_size, bias=bias)\n",
    "        self.output_layer = nn.Linear(self.embz_size, self.output_size, bias=bias)\n",
    "        \n",
    "        #set tied weights: three way tied weights vs two way tied weights\n",
    "        if self.tied_weight_type == 'three_way':\n",
    "            self.decoder_embedding_layer.weight  = self.encoder_embedding_layer.weight\n",
    "            self.output_layer.weight = self.decoder_embedding_layer.weight  \n",
    "        else:\n",
    "            if self.pre_trained_vector: self.decoder_embedding_layer.weight.data.copy_(self.pre_trained_vector.weight.data)\n",
    "            self.output_layer.weight = self.decoder_embedding_layer.weight  \n",
    "            \n",
    "        #set attention\n",
    "        self.encoder_output_layer = nn.Linear(self.hidden_size*self.num_directions, self.embz_size, bias=bias)\n",
    "        self.att_vector_layer = nn.Linear(self.embz_size+self.embz_size, self.embz_size,bias=bias)\n",
    "        if self.attention_type == 'Bahdanau':\n",
    "            self.decoder_hidden_layer = nn.Linear(self.hidden_size*self.num_directions, self.embz_size, bias=bias)\n",
    "            self.att_score = nn.Linear(self.embz_size,1,bias=bias)\n",
    "\n",
    "            \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (V(torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size)),\n",
    "                    V(torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size)))\n",
    "        else:\n",
    "            return V(torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size))\n",
    "   \n",
    "\n",
    "    def _cat_directions(self, hidden):\n",
    "        def _cat(h):\n",
    "            return torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2)\n",
    "            \n",
    "        if isinstance(hidden, tuple):\n",
    "            # LSTM hidden contains a tuple (hidden state, cell state)\n",
    "            hidden = tuple([_cat(h) for h in hidden])\n",
    "        else:\n",
    "            # GRU hidden\n",
    "            hidden = _cat(hidden)\n",
    "        return hidden    \n",
    "    \n",
    "    \n",
    "    def bahdanau_attention(self, encoder_output, decoder_hidden, decoder_input):\n",
    "        encoder_output = self.encoder_output_layer(encoder_output) \n",
    "        encoder_output = encoder_output.transpose(0,1)\n",
    "        decoder_hidden = decoder_hidden.transpose(0,1)\n",
    "        att_score = F.tanh(encoder_output + decoder_hidden)\n",
    "        att_score = self.att_score(att_score)\n",
    "        att_weight = F.softmax(att_score, dim=1)\n",
    "        context_vector = torch.bmm(att_weight.transpose(-1, 1), encoder_output).squeeze(1)\n",
    "        att_vector = torch.cat((context_vector, decoder_input), dim=1)\n",
    "        att_vector = self.att_vector_layer(att_vector)\n",
    "        return att_weight.squeeze(-1), att_vector\n",
    "    \n",
    "    \n",
    "    def luong_attention(self, encoder_output, decoder_output):\n",
    "        encoder_output = self.encoder_output_layer(encoder_output) \n",
    "        encoder_output = encoder_output.transpose(0,1)\n",
    "        decoder_output = decoder_output.transpose(0,1)\n",
    "        att_score = torch.bmm(encoder_output, decoder_output.transpose(-1,1))\n",
    "        att_weight = F.softmax(att_score, dim=1)\n",
    "        context_vector = torch.bmm(att_weight.transpose(-1, 1), encoder_output).squeeze(1)\n",
    "        att_vector = torch.cat((context_vector, decoder_output.squeeze(1)), dim=1)\n",
    "        att_vector = self.att_vector_layer(att_vector)\n",
    "        att_vector = F.tanh(att_vector)\n",
    "        return att_weight.squeeze(-1), att_vector\n",
    "        \n",
    "    def decoder_forward(self, batch_size, encoder_output, decoder_hidden, y=None):\n",
    "        decoder_input = V(torch.zeros(batch_size).long())  \n",
    "        output_seq_stack, att_stack = [], []\n",
    "        \n",
    "        for i in range(self.max_tgt_len):\n",
    "            decoder_input = self.decoder_dropout(self.decoder_embedding_layer(decoder_input))\n",
    "            if self.attention_type == 'Bahdanau':\n",
    "                if isinstance(decoder_hidden, tuple):\n",
    "                    prev_hidden = self.decoder_hidden_layer(decoder_hidden[0][-1]).unsqueeze(0)\n",
    "                else:\n",
    "                    prev_hidden = self.decoder_hidden_layer(decoder_hidden[-1]).unsqueeze(0) \n",
    "                att, decoder_input = self.bahdanau_attention(encoder_output, prev_hidden, decoder_input)\n",
    "                decoder_output, decoder_hidden = self.decoder_rnn(decoder_input.unsqueeze(0), decoder_hidden)\n",
    "                decoder_output = self.decoder_output_layer(decoder_output.squeeze(0)) \n",
    "            else:\n",
    "                decoder_output, decoder_hidden = self.decoder_rnn(decoder_input.unsqueeze(0), decoder_hidden)\n",
    "                decoder_output = self.decoder_output_layer(decoder_output) \n",
    "                att, decoder_output = self.luong_attention(encoder_output, decoder_output)\n",
    "            att_stack.append(att)\n",
    "            output = self.output_layer(decoder_output)\n",
    "            output_seq_stack.append(output)\n",
    "            decoder_input = V(output.data.max(1)[1])\n",
    "            if (decoder_input==1).all(): break \n",
    "            if self.teacher_forcing:    \n",
    "                samp_prob = round(random.random(),1)\n",
    "                if (y is not None) and (samp_prob < self.force_prob):\n",
    "                    if i >= len(y): break\n",
    "                    decoder_input = y[i] \n",
    "                \n",
    "        return torch.stack(output_seq_stack), torch.stack(att_stack)\n",
    "        \n",
    "                \n",
    "    def forward(self, seq, y=None):\n",
    "        batch_size = seq[0].size(0)\n",
    "        encoder_hidden = self.init_hidden(batch_size)\n",
    "        encoder_input = self.encoder_dropout(self.encoder_embedding_layer(seq))\n",
    "        encoder_output, encoder_hidden = self.encoder_rnn(encoder_input, encoder_hidden) \n",
    "        if self.bidirectional:\n",
    "            encoder_hidden = self._cat_directions(encoder_hidden)\n",
    "        output = self.decoder_forward(batch_size, encoder_output, encoder_hidden, y=y)\n",
    "        if isinstance(encoder_hidden, tuple):\n",
    "            encoder_vector = self.encoder_vector_layer(encoder_hidden[0][-1])\n",
    "        else:\n",
    "            encoder_vector = self.encoder_vector_layer(encoder_hidden[-1])\n",
    "        output = output + (encoder_vector,)  \n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set maximum target summary size \n",
    "its = [next(model_data.trn_dl.__iter__())[1] for i in range(10)]\n",
    "max_tgt_len = int(np.percentile([its[o].size()[0] for o in range(len(its))], 99))\n",
    "max_tgt_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(path, filename, file):\n",
    "    \"\"\"Function to save file as pickle\"\"\"\n",
    "    with open(f'{path}/{filename}', 'wb') as f:\n",
    "        pickle.dump(file, f)\n",
    "\n",
    "        \n",
    "def norm_pre_trained_embeddings(vecs, itos, em_sz, padding_idx):\n",
    "    \"\"\"Function to load and normalize pretrained vectors\"\"\"\n",
    "    emb = nn.Embedding(len(itos), em_sz, padding_idx=padding_idx)\n",
    "    wgts = emb.weight.data\n",
    "    for i,w in enumerate(itos):\n",
    "        try: \n",
    "            wgts[i] = torch.from_numpy(vecs[w]-vec_mean)\n",
    "            wgts[i] = torch.from_numpy(vecs[w]/vec_std)\n",
    "        except: pass \n",
    "    emb.weight.requires_grad = False    \n",
    "    return emb\n",
    "\n",
    "\n",
    "def embedding_param(path, data_field, pre_trained_vector_type, embz_size=128, save_vocab=False, itos='itos', stoi='stoi'):\n",
    "    \"\"\"Returns embedding parameters\"\"\"\n",
    "    pre_trained=None\n",
    "    padding_idx = data_field.vocab.stoi['<pad>']\n",
    "    index_to_string, string_to_index = data_field.vocab.itos, data_field.vocab.stoi\n",
    "    if save_vocab:\n",
    "        vocab_path = os.path.join(path, \"vocab\")\n",
    "        os.makedirs(vocab_path, exist_ok=True)\n",
    "        save_pickle(vocab_path, f'{itos}.pk', index_to_string) \n",
    "        save_pickle(vocab_path, f'{stoi}.pk', string_to_index) \n",
    "    if pre_trained_vector_type:\n",
    "        vec_mean, vec_std = data_field.vocab.vectors.numpy().mean(), data_field.vocab.vectors.numpy().std()\n",
    "        print('pre_trained_vector_mean = %s, pre_trained_vector_std = %s'%(vec_mean, vec_std))\n",
    "        vector_weight_matrix = data_field.vocab.vectors\n",
    "        embz_size = vector_weight_matrix.size(1)\n",
    "        pre_trained = norm_pre_trained_embeddings(vector_weight_matrix, index_to_string, embz_size, padding_idx)\n",
    "        print('Normalizing.... \\npre_trained_vector_mean = %s, pre_trained_vector_std = %s' %(pre_trained.weight.data.numpy().mean(), pre_trained.weight.data.numpy().std()))\n",
    "    return pre_trained, embz_size, padding_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rev = 1\n"
     ]
    }
   ],
   "source": [
    "rev=0\n",
    "rev += 1\n",
    "print(\"rev = %s\" %rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_trained_vector_mean = 0.0019763561, pre_trained_vector_std = 0.43612698\n",
      "Normalizing.... \n",
      "pre_trained_vector_mean = -0.0003179377, pre_trained_vector_std = 1.0001361\n"
     ]
    }
   ],
   "source": [
    "pre_trained_vector,  embz_size, padding_idx = embedding_param(SAMPLE_DATA_PATH, TEXT, pre_trained_vector_type, save_vocab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(TEXT.vocab)\n",
    "hidden_size = 400\n",
    "output_size =  len(TEXT.vocab)\n",
    "rnn_type = 'gru'\n",
    "tied_weight_type ='three_way'\n",
    "max_tgt_len = max_tgt_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from fastai.model import Stepper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqStepper(Stepper):\n",
    "    def step(self, xs, y, epoch):\n",
    "        output = self.m(*xs, y)\n",
    "        xtra = []\n",
    "        if isinstance(output,tuple): output,*xtra = output\n",
    "        self.opt.zero_grad()\n",
    "        loss = raw_loss = self.crit(output, y)\n",
    "        if self.reg_fn: loss = self.reg_fn(output, xtra, raw_loss)\n",
    "        loss.backward()\n",
    "        if self.clip:   # Gradient clipping\n",
    "            nn.utils.clip_grad_norm(trainable_params_(self.m), self.clip)\n",
    "        self.opt.step()\n",
    "        return raw_loss.data\n",
    "    \n",
    "def seq2seq_loss(input, target):\n",
    "    sl,bs = target.size()\n",
    "    sl_in,bs_in,nc = input.size()\n",
    "    if sl>sl_in: input = F.pad(input, (0,0,0,0,0,sl-sl_in))\n",
    "    input = input[:sl]\n",
    "    return F.cross_entropy(input.view(-1,nc), target.view(-1))#, ignore_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom callbacks\n",
    "from tensorboardX import SummaryWriter\n",
    "from fastai.sgdr import Callback, DecayScheduler\n",
    "from fastai.learner import Learner\n",
    "\n",
    "class TensorboardLogger(Callback):\n",
    "    def __init__(self, path, log_name, metrics_names=[]):\n",
    "        super().__init__()\n",
    "        self.metrics_names = [\"validation_loss\"]\n",
    "        self.metrics_names += metrics_names\n",
    "        log_path = os.path.join(path, \"logs\")\n",
    "        self.log_dir = os.path.join(log_path, log_name)\n",
    "        if os.path.exists(self.log_dir): shutil.rmtree(self.log_dir)\n",
    "        os.makedirs(self.log_dir)\n",
    "        \n",
    "    def on_train_begin(self):\n",
    "        self.iteration = 0\n",
    "        self.epoch = 0\n",
    "        self.writer = SummaryWriter(log_dir=self.log_dir)\n",
    "    def on_batch_begin(self): pass\n",
    "    def on_phase_begin(self): pass\n",
    "    def on_epoch_end(self, metrics):\n",
    "        self.epoch += 1\n",
    "        for val, name in zip(metrics, self.metrics_names):\n",
    "            self.writer.add_scalar(name, val, self.iteration) \n",
    "                        \n",
    "    def on_phase_end(self): pass\n",
    "    def on_batch_end(self, loss):\n",
    "        self.iteration += 1\n",
    "        self.writer.add_scalar(\"training_loss\", loss, self.iteration)\n",
    "    def on_train_end(self):\n",
    "        self.writer.close()\n",
    "        \n",
    "        \n",
    "class BestModelCheckPoint(Callback):\n",
    "    def __init__(self, learner, path, model_name, lr):\n",
    "        super().__init__()\n",
    "        self.learner = learner\n",
    "        self.model_name = model_name\n",
    "        self.learning_rate = lr\n",
    "        self.model_log = {}\n",
    "        self.model_path = self.learner.models_path\n",
    "        os.makedirs(self.model_path, exist_ok=True)\n",
    "\n",
    "    def on_train_begin(self): \n",
    "        self.first_epoch = True\n",
    "        self.epoch = 0\n",
    "        self.best_loss = 0.\n",
    "\n",
    "    def on_batch_begin(self): pass\n",
    "    def on_phase_begin(self): pass\n",
    "    def on_epoch_end(self, metrics): \n",
    "        self.epoch += 1\n",
    "        self.val_loss = metrics[0]\n",
    "        if self.first_epoch:\n",
    "            self.best_loss = self.val_loss\n",
    "            self.first_epoch = False\n",
    "        elif self.val_loss < self.best_loss:\n",
    "            self.best_loss = self.val_loss\n",
    "            self.learner.save(self.model_name)\n",
    "            self.model_log['training_loss'] = [str(self.train_losses)]\n",
    "            self.model_log['validation_loss'] = [str(self.val_loss)]\n",
    "            self.model_log['epoch_num'] = [str(self.epoch)]\n",
    "            self.model_log['learning_rate'] = [str(self.learning_rate)]\n",
    "            self.model_log['model_info'] = [w for s in [str(self.learner.model)] for w in s.split('\\n')]\n",
    "            self.model_log['model_info'].append(\"(attention_type): %s\" %self.learner.model.attention_type)\n",
    "            self.model_log['model_info'].append(\"(weight_tie): %s\" %self.learner.model.tied_weight_type)\n",
    "            self.model_log['model_info'].append(\"(pre_trained_vector_type): %s\" %self.learner.model.pre_trained_vector_type)\n",
    "            self.model_log['model_info'].append(\"(teacher_forcing): %s\" %self.learner.model.teacher_forcing)\n",
    "            if self.learner.model.teacher_forcing: self.model_log['model_info'].append(\"(teacher_forcing_prob): %s\" %self.learner.model.force_prob)\n",
    "            with open(f'{self.model_path}/{self.model_name}_model_log.json', 'w') as d: json.dump(self.model_log, d)\n",
    "        else: pass        \n",
    "    def on_phase_end(self): pass\n",
    "    def on_batch_end(self, loss):\n",
    "        self.train_losses = loss\n",
    "    def on_train_end(self): \n",
    "            self.learner.save(f'{self.model_name}_train_end')\n",
    "            self.model_log['training_loss'] = [str(self.train_losses)]\n",
    "            self.model_log['validation_loss'] = [str(self.val_loss)]\n",
    "            self.model_log['epoch_num'] = [str(self.epoch)]\n",
    "            self.model_log['learning_rate'] = [str(self.learning_rate)]\n",
    "            self.model_log['model_info'] = [w for s in [str(self.learner.model)] for w in s.split('\\n')]\n",
    "            self.model_log['model_info'].append(\"(attention_type): %s\" %self.learner.model.attention_type)\n",
    "            self.model_log['model_info'].append(\"(weight_tie): %s\" %self.learner.model.tied_weight_type)\n",
    "            self.model_log['model_info'].append(\"(pre_trained_vector_type): %s\" %self.learner.model.pre_trained_vector_type)\n",
    "            self.model_log['model_info'].append(\"(teacher_forcing): %s\" %self.learner.model.teacher_forcing)\n",
    "            if self.learner.model.teacher_forcing: self.model_log['model_info'].append(\"(teacher_forcing_prob): %s\" %self.learner.model.force_prob)\n",
    "            with open(f'{self.model_path}/{self.model_name}_train_end_model_log.json', 'w') as d: json.dump(self.model_log, d)\n",
    "\n",
    "class TeacherForcingSched(Callback):\n",
    "    def __init__(self, learner, scheduler):\n",
    "        super().__init__()\n",
    "        self.learner = learner\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "    def on_train_begin(self): \n",
    "        self.learner.model.force_prob = round(self.scheduler.next_val(),1)\n",
    "        \n",
    "    def on_batch_begin(self): pass\n",
    "    def on_phase_begin(self): pass\n",
    "    def on_epoch_end(self, metrics): \n",
    "        self.learner.model.force_prob = round(self.scheduler.next_val(),1)\n",
    "        \n",
    "    def on_phase_end(self): pass\n",
    "    def on_batch_end(self, loss):pass\n",
    "    def on_train_end(self): pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Model log:\n",
      "Seq2SeqRNN(\n",
      "  (pre_trained_vector): Embedding(52299, 200, padding_idx=1)\n",
      "  (encoder_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (encoder_embedding_layer): Embedding(52299, 200, padding_idx=1)\n",
      "  (encoder_rnn): GRU(200, 200, bidirectional=True)\n",
      "  (encoder_vector_layer): Linear(in_features=400, out_features=200, bias=False)\n",
      "  (decoder_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (decoder_embedding_layer): Embedding(52299, 200, padding_idx=1)\n",
      "  (decoder_rnn): GRU(200, 400)\n",
      "  (decoder_output_layer): Linear(in_features=400, out_features=200, bias=False)\n",
      "  (output_layer): Linear(in_features=200, out_features=52299, bias=False)\n",
      "  (encoder_output_layer): Linear(in_features=400, out_features=200, bias=False)\n",
      "  (att_vector_layer): Linear(in_features=400, out_features=200, bias=False)\n",
      ") \n",
      "\n",
      "- attention_type = Luong \n",
      "\n",
      "- weight_tie = three_way \n",
      "\n",
      "- teacher_forcing = True \n",
      " \n",
      "- pre_trained_embedding = glove.6B.200d \n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attention_type='luong'\n",
    "model_luong = Seq2SeqRNN(rnn_type, input_size, embz_size, hidden_size, batch_size, output_size, max_tgt_len,\n",
    "               attention_type, tied_weight_type, pre_trained_vector, pre_trained_vector_type, padding_idx)\n",
    "\n",
    "print('='*100)\n",
    "print('Model log:')\n",
    "print(model_luong, '\\n')\n",
    "print('- attention_type = {} \\n'.format(model_luong.attention_type))\n",
    "print('- weight_tie = {} \\n'.format(model_luong.tied_weight_type))\n",
    "print('- teacher_forcing = {} \\n '.format(model_luong.teacher_forcing)) \n",
    "print('- pre_trained_embedding = {} \\n'.format(model_luong.pre_trained_vector_type)) \n",
    "print('='*100 + '\\n')\n",
    "\n",
    "if USE_GPU:\n",
    "    model_luong.cuda()\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "learn_luong = RNN_Learner(model_data, SingleModel(model_luong), opt_fn=opt_fn)\n",
    "learn_luong.crit = seq2seq_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4ffb36b24740ffbdfc16dd0ec5547a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 817/1250 [1:06:22<17:42,  2.45s/it, loss=31.1]  "
     ]
    }
   ],
   "source": [
    "learn_luong.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-bca06a957c50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn_luong\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'plot'"
     ]
    }
   ],
   "source": [
    "learn_luong.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249036fe4b744394a2e4bd8065ce6a93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                            \n",
      "    0      5.373187   5.754892  \n",
      " 20%|█▉        | 244/1250 [12:19<56:09,  3.35s/it, loss=tensor(5.5522)] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-6d18ee65316c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mteach_forcer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTeacherForcingSched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn_luong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m learn_luong.fit(lr, 1, cycle_len=cycle_len, use_clr=(20,10), stepper=Seq2SeqStepper, \\\n\u001b[0;32m---> 10\u001b[0;31m           callbacks=[tb_logger, teach_forcer, best_model])\n\u001b[0m",
      "\u001b[0;32m~/Documents/GitHub/fastai/old/fastai/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_crit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/fastai/old/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, lrs, n_cycle, wds, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mlayer_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cycle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwarm_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/fastai/old/fastai/learner.py\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp16\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mswa_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswa_model\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_swa\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswa_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswa_start\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             swa_eval_freq=swa_eval_freq, **kwargs)\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/fastai/old/fastai/model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, visualize, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mbatch_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_stepper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mavg_mom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mavg_mom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mdebias_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavg_mom\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-b264c1c16ada>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, xs, y, epoch)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_params_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Luong Attention model\n",
    "lr=1e-3\n",
    "model_name = f'{model_luong.rnn_type}_{model_luong.attention_type}_rev_{rev}'.lower()\n",
    "cycle_len=15\n",
    "best_model = BestModelCheckPoint(learn_luong, model_data.path, model_name, lr)\n",
    "tb_logger = TensorboardLogger(model_data.path, model_name)\n",
    "sched = DecayScheduler(DecayType.LINEAR, cycle_len, 0.5, 0.1)\n",
    "teach_forcer = TeacherForcingSched(learn_luong, sched)\n",
    "learn_luong.fit(lr, 1, cycle_len=cycle_len, use_clr=(20,10), stepper=Seq2SeqStepper, \\\n",
    "          callbacks=[tb_logger, teach_forcer, best_model])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Model log:\n",
      "Seq2SeqRNN(\n",
      "  (pre_trained_vector): Embedding(52299, 200, padding_idx=1)\n",
      "  (encoder_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (encoder_embedding_layer): Embedding(52299, 200, padding_idx=1)\n",
      "  (encoder_rnn): GRU(200, 200, bidirectional=True)\n",
      "  (encoder_vector_layer): Linear(in_features=400, out_features=200, bias=False)\n",
      "  (decoder_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (decoder_embedding_layer): Embedding(52299, 200, padding_idx=1)\n",
      "  (decoder_rnn): GRU(200, 400)\n",
      "  (decoder_output_layer): Linear(in_features=400, out_features=200, bias=False)\n",
      "  (output_layer): Linear(in_features=200, out_features=52299, bias=False)\n",
      "  (encoder_output_layer): Linear(in_features=400, out_features=200, bias=False)\n",
      "  (att_vector_layer): Linear(in_features=400, out_features=200, bias=False)\n",
      "  (decoder_hidden_layer): Linear(in_features=400, out_features=200, bias=False)\n",
      "  (att_score): Linear(in_features=200, out_features=1, bias=False)\n",
      ") \n",
      "\n",
      "- attention_type = Bahdanau \n",
      "\n",
      "- weight_tie = three_way \n",
      "\n",
      "- teacher_forcing = True \n",
      " \n",
      "- pre_trained_embedding = glove.6B.200d \n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "attention_type='bahdanau'\n",
    "model_bahdanau = Seq2SeqRNN(rnn_type, input_size, embz_size, hidden_size, batch_size, output_size, max_tgt_len,\n",
    "               attention_type, tied_weight_type, pre_trained_vector, pre_trained_vector_type, padding_idx)\n",
    "\n",
    "print('='*100)\n",
    "print('Model log:')\n",
    "print(model_bahdanau, '\\n')\n",
    "print('- attention_type = {} \\n'.format(model_bahdanau.attention_type))\n",
    "print('- weight_tie = {} \\n'.format(model_bahdanau.tied_weight_type))\n",
    "print('- teacher_forcing = {} \\n '.format(model_bahdanau.teacher_forcing)) \n",
    "print('- pre_trained_embedding = {} \\n'.format(model_bahdanau.pre_trained_vector_type)) \n",
    "print('='*100 + '\\n')\n",
    "\n",
    "if USE_GPU:\n",
    "    model_bahdanau.cuda()\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "learn_bahdanau = RNN_Learner(model_data, SingleModel(model_bahdanau), opt_fn=opt_fn)\n",
    "learn_bahdanau.crit = seq2seq_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be91ea353e8f4c6abb015f10c70b3912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                            \n",
      "    0      7.833976   8.051148  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.05114815120697]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bahdanau Attention Model\n",
    "lr=1e-3\n",
    "model_name = f'{model_bahdanau.rnn_type}_{model_bahdanau.attention_type}_rev_{rev}'.lower()\n",
    "cycle_len=1\n",
    "best_model = BestModelCheckPoint(learn_bahdanau, model_data.path, model_name, lr)\n",
    "tb_logger = TensorboardLogger(model_data.path, model_name)\n",
    "sched = DecayScheduler(DecayType.LINEAR, cycle_len, 0.5, 0.1)\n",
    "teach_forcer = TeacherForcingSched(learn_bahdanau, sched)\n",
    "learn_bahdanau.fit(lr, 1, cycle_len=cycle_len, use_clr=(20,10), stepper=Seq2SeqStepper, \\\n",
    "          callbacks=[tb_logger, teach_forcer, best_model])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    fontdict = {'fontsize': 14}\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def generate(x, y, m):\n",
    "    probs = m.model(V(x))\n",
    "    preds, attention, encoder_embedding = to_np(probs[0].max(2)[1]), to_np(probs[1].squeeze(1)), to_np(probs[2])\n",
    "    sentence = ' '.join([index_to_string[o] for o in x[:,0].data.cpu().numpy() if o != 1])\n",
    "    result = ' '.join([index_to_string[o] for o in preds[:,0] if o!=1])\n",
    "    orig = ' '.join([index_to_string[o] for o in y[:,0].data.cpu().numpy() if o != 1])\n",
    "    print('Input: {}'.format(sentence), '\\n')\n",
    "    print('Original summary: {}'.format(orig), '\\n')\n",
    "    print('Predicted summary: {}'.format(result))\n",
    "    attention_plot = attention[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
    "    return preds, attention, encoder_embedding\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "attention_type='luong'\n",
    "model_luong = Seq2SeqRNN(rnn_type, input_size, embz_size, hidden_size, batch_size, output_size, max_tgt_len,\n",
    "               attention_type, tied_weight_type, pre_trained_vector, pre_trained_vector_type, padding_idx)\n",
    "if USE_GPU:\n",
    "    model_luong.cuda()\n",
    "learn_luong = RNN_Learner(model_data, SingleModel(model_luong))\n",
    "learn_luong.load('gru_luong_rev_1_train_end')\n",
    "'''\n",
    "\n",
    "attention_type='bahdanau'\n",
    "model_bahdanau = Seq2SeqRNN(rnn_type, input_size, embz_size, hidden_size, batch_size, output_size, max_tgt_len,\n",
    "               attention_type, tied_weight_type, pre_trained_vector, pre_trained_vector_type, padding_idx)\n",
    "if USE_GPU:\n",
    "    model_bahdanau.cuda()\n",
    "learn_bahdanau = RNN_Learner(model_data, SingleModel(model_bahdanau))\n",
    "learn_bahdanau.load('gru_bahdanau_rev_1_train_end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'index_to_string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-f6cc15199fe0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_luong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-9e212800fb9a>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(x, y, m)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_to_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_to_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0morig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_to_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-9e212800fb9a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_to_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_to_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0morig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_to_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index_to_string' is not defined"
     ]
    }
   ],
   "source": [
    "# Luong (Global Dot) Attention\n",
    "x,y = next(iter(model_data.trn_dl))\n",
    "for i in range(1):\n",
    "    print(i)\n",
    "    preds, attention, encoder_embedding = generate(x.transpose(1,0)[i].unsqueeze(1), y.transpose(1,0)[i].unsqueeze(1), learn_luong)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'index_to_string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-7934242f5807>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_bahdanau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-9e212800fb9a>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(x, y, m)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_to_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_to_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0morig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_to_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-9e212800fb9a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_to_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_to_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0morig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_to_string\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index_to_string' is not defined"
     ]
    }
   ],
   "source": [
    "# Bahdanau Attention\n",
    "for i in range(1):\n",
    "    print(i)\n",
    "    preds, attention, encoder_embedding = generate(x.transpose(1,0)[i].unsqueeze(1), y.transpose(1,0)[i].unsqueeze(1), learn_bahdanau)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
